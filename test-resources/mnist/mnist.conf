# config

[default]
root_dir = ${env:app_root}
test_resources = ${root_dir}/test-resources/mnist
temporary_dir = ${root_dir}/target/mnist
temporary_batch_dir = ${temporary_dir}/batch
results_dir = ${temporary_dir}/results
dataset_dir = ${root_dir}/datasets/mnist

[torch_config]
class_name = zensols.deeplearn.TorchConfig
use_gpu = False
data_type = eval({'import': ['torch']}): torch.float32

[gpu_torch_config]
class_name = zensols.deeplearn.TorchConfig
use_gpu = True
data_type = eval({'import': ['torch']}): torch.float32


# vectorizer
[identity_vectorizer]
class_name = zensols.deeplearn.vectorize.IdentityEncodableFeatureVectorizer
feature_id = identity

[mnist_vectorizer_manager]
class_name = zensols.deeplearn.vectorize.FeatureVectorizerManager
torch_config = instance: torch_config
configured_vectorizers = eval: ['identity_vectorizer']

[vectorizer_set]
class_name = zensols.deeplearn.vectorize.FeatureVectorizerManagerSet
names = eval: 'mnist_vectorizer_manager'.split()


# stashes

[dataloader_stash]
class_name = mnist.stash.DataLoaderStash
delegate = eval({'import': ['zensols.persist']}): zensols.persist.DictionaryStash()
path = ${default:dataset_dir}

[dataset_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: dataloader_stash
split_container = instance: dataloader_stash
sort_function = eval: int

[batch_dataset_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:temporary_batch_dir}/data

[mnist_batch_dataframe_stash]
class_name = zensols.deeplearn.batch.BatchStash
delegate = instance: batch_dataset_dir_stash
split_stash_container = instance: dataset_stash
data_point_id_sets_path = path: ${default:temporary_batch_dir}/batch-point-keys.dat
vectorizer_manager_set = instance: vectorizer_set
data_point_type = eval({'import': ['mnist.model']}): mnist.model.MnistDataPoint
batch_type = eval({'import': ['mnist.model']}): mnist.model.MnistBatch
decoded_attributes = None
model_torch_config = instance: gpu_torch_config
batch_size = 20
chunk_size = 0
workers = 0
batch_limit = eval: sys.maxsize

[mnist_batch_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: mnist_batch_dataframe_stash
split_container = instance: mnist_batch_dataframe_stash
sort_function = eval: int

# model

[net_settings]
class_name = mnist.model.MnistNetworkSettings

[model_settings]
class_name = zensols.deeplearn.model.ModelSettings
path = path: ${default:temporary_dir}/model
learning_rate = 0.01
# keep all data in GPU memory
batch_iteration = gpu
# number of epochs to train the model
epochs = 40
# set in the notebook as an example, which boosts performance
#optimizer_class_name = torch.optim.SGD

[executor]
class_name = zensols.deeplearn.model.ModelExecutor
model_name = Mnist
model_settings = instance: model_settings
net_settings = instance: net_settings
dataset_stash = instance: mnist_batch_stash
dataset_split_names = eval: 'train val test'.split()
result_path = path: ${default:results_dir}
