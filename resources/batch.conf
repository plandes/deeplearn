## Batches used for training the model.  This configures the classes that
## groups each data time in to files for fast retrieval later.

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
[batch_dir_stash]
class_name = zensols.deeplearn.batch.BatchDirectoryCompositeStash
# top level directory to store each feature sub directory
path = path: ${deeplearn_default:batch_dir}/${deeplearn_default:batch_size}-vector
# feature grouping: when at least one in a group is needed, all of the features
# in that group are loaded
groups = eval: (
   # there will be N (deeplearn_default:batch_size) batch labels in one file in a
   # directory of just label files; more features can/should follow
   set('label'.split()),)


# a stash of Batch instances given to the model during training, validation and
# testing; this class spawns sub processes to concatenate arrays of features in
# to batches containing the tensors consumed by the model
[batch_stash]
class_name = zensols.deeplearn.batch.BatchStash
#
# the backing stash, which in this case, is the composite stash that loads
# groups of features at a time
delegate = instance: batch_dir_stash
#
# this stash is used to generate instances of what will be used to create batches
#split_stash_container = instance: feature_stash
#
# where to store the keys as mutually exclusive across dataset (train vs test etc)
data_point_id_sets_path = path: ${deeplearn_default:batch_dir}/${deeplearn_default:batch_size}-keys.dat
#
# indicate what will be used to vectorize in to tensors from features
vectorizer_manager_set = instance: vectorizer_manager_set
#
# the class that contains the feature data, one for each data instance
#data_point_type = eval({'import': ['<module name>']}): <module name>.YourClassDataPoint
#
# the class taht contains the batch data, which will have N instances of
# `data_point_type` where N is the `batch_size`
batch_type = eval({'import': ['zensols.deeplearn.batch']}): zensols.deeplearn.batch.DefaultBatch
#
# train time tweekable attributes used only on loading; all features indicated
# in the vectorizer manager (i.e. `language_feature_manager`) are saved; this
# makes it fast to try different feature combinations without havign to
# re-vectorize the entire dataset; if this is set to `None`, use all attributes
# given
#decoded_attributes = set: label, ${deeplearn_default:lang_features} ${deeplearn_default:embedding}
decoded_attributes = set: label
#
# dependencies, enums
# the PyTorch configuration used to load batch data in to memory, in this case,
# the GPU if available
model_torch_config = instance: gpu_torch_config
#
# number of chunks of data given to each worker sub process; if 0 optimize for
# batch size and number of CPU cores
chunk_size = 0
# number sub processes; if 0, then the number of CPU cores; if a negative
# number the number of cores minus this value
workers = 0
#
# the number of data instances per batch, and the first dimension of each
# tensor given to the model (moved to defaults so batch_dir_stash:path is
# configurable)
batch_size = ${deeplearn_default:batch_size}
#
# limit on the number of batches (when creating batchs) per data set; typically
# multiply this by 3 to get a total count
#batch_limit = 3


# stash to not only split data by dataset (i.e. train, test), but also sort the
# keys across all; which is important for reproducibility of results; this
# pulls from the `batch_stash`, so sorting is done only on the loaded data
#
# note: even if the `feature_stash` (batch_stash:split_stash_container) has
# sorted keys, we still need to sort the keys again since feature instances are
# many-to-one with batches and their respective unique keys
[dataset_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: batch_stash
split_container = instance: batch_stash
sort_function = eval: int
