## Model configuration, settings and hyperparameters

# the executor uses PyTorch to train, test and validate the model itself; it
# also saves the results and model
[executor]
class_name = zensols.deeplearn.model.ModelExecutor
#
# configures the model
model_settings = instance: model_settings
#
# configures the neural network
#net_settings = instance: net_settings
#
# stash to get the batch data
dataset_stash = instance: dataset_stash
#
# the datasets by name found in `dataset_stash`
# *Important:* order must be: training, validation/development, test
dataset_split_names = eval: 'train validation test'.split()
#
# the path the store the results after completing training or testing
result_path = path: ${deeplearn_default:results_dir}/model
#
# add results while training the model
intermediate_results_path = path: ${deeplearn_default:temporary_dir}/tmp
#
# the path to watch for early stopping
update_path = path: ${deeplearn_default:temporary_dir}/update.json
#
# cross-fold result path
#cross_fold_path = path: ${deeplearn_default:results_dir}/cross-fold



# model configuration
[recurrent_settings]
class_name = zensols.deeplearn.layer.RecurrentAggregationNetworkSettings
#
# gpu layer configuration
torch_config = instance: ${deeplearn_default:layer_torch_config}
#
# the type of network (one of `rnn`, `lstm`, `gru`)
network_type = lstm
#
# the type of aggregation of the layer, one of `max`, `ave`, `last`
aggregation = max
#
# the input size, but set to None since this is set from the embedding layer
# metadata
input_size = None
#
# hidden LSTM dimension
hidden_size = 8
#
# "stacked" LSTM
num_layers = 1
#
# whether or the LSTM is stacked
bidirectional = True
#
# set by root level settings
dropout = None


[linear_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
#
# number deep linear layers, each element as the number of parameters
middle_features = eval: []
#
# number of output features
#--config note: uncomment, but be careful not to inclue a ``list:``
#out_features = eval: len('${label_vectorizer:categories}'.split())
#
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
#
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
#
# number of times to repeat the middle layers
repeats = 1
#
# drop out used for the middle layers (set by root level settings)
dropout = None
#
# no activation used in this set of layers
activation = None
#
# the dimension of the batch normalization to use or None
batch_norm_d = None
#
# number of features C used by the norm or `None` if not used; where C from an
# expected input of size (N, C, L) or L from input of size (N, L)
batch_norm_features = None
# the GPU configuration for Torch Module layers
torch_config = instance: ${deeplearn_default:layer_torch_config}


# model specific configuration, mutually exclusive from neural network details
[model_settings]
class_name = zensols.deeplearn.model.ModelSettings
# human readable text for the model name
model_name = ${deeplearn_default:model_name}
# path where the model is saved on each validation decrease
path = path: ${deeplearn_default:temporary_dir}
# append the a normalized string of 'model_name' to the end 'path'
append_model_path = normalize
#
# learning rate set on the optimizer
#learning_rate = 0.001
#
# how the batches are buffered; one of `gpu`, which buffers all data in the
# GPU, `cpu`, which means keep all batches in CPU memory (the default), or
# `buffered` which means to buffer only one batch at a time (only for *very*
# large data) how to batch data: gpu to load all in to the GPU,
batch_iteration = gpu
# if True, cache unthawed/processed batches when possible; which must be False
# if batch_iteration = buffered
cache_batches = True
#
# number of epochs to train the model
epochs = 2
#
# the maximum number of times the validation loss can decrease per epoch before
# the executor "gives up" and ends training
#max_consecutive_increased_count = 5
#
# indicates the frequency by with the Python garbage collector should be
# invoked: 0: never 1: before and after training or testing 2: after each epoch
# 3: after each batch
#gc_level = 1
#
# an optional factory used to create predictions
#prediction_mapper_name = feature_prediction_mapper
#
# optimizer / loss
#optimizer_class_name = torch.optim.SGD
#criterion_class_name = torch.nn.MSELoss
#
# learning rate scheduler
scheduler_class_name = torch.optim.lr_scheduler.ReduceLROnPlateau
#
# number of batches to limit for train, test and validation, which is used for
# debugging
#batch_limit = 2
